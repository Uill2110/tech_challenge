{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f576554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import custom modules\n",
    "import sys\n",
    "sys.path.append('../src') # Add src directory to Python path\n",
    "from data_loader import DataLoader\n",
    "from preprocessor import DataPreprocessor\n",
    "from models import ModelTrainer\n",
    "from evaluator import ModelEvaluator\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e026ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 1. Data Loading and Initial Exploration ---\")\n",
    "loader = DataLoader()\n",
    "df = loader.load_sample_data('diabetes')\n",
    "loader.get_info()\n",
    "\n",
    "# Visualize target distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='Outcome', data=df)\n",
    "plt.title('Distribution of Target Variable (0: No Diabetes, 1: Diabetes)')\n",
    "plt.show()\n",
    "\n",
    "# Visualize a few feature distributions (example)\n",
    "# Add more plots as discussed in the project requirements\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['glucose'], kde=True, bins=30)\n",
    "plt.title('Distribution of Glucose')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df['bmi'], kde=True, bins=30)\n",
    "plt.title('Distribution of BMI')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a3e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 2. Data Preprocessing ---\")\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = preprocessor.split_data(df, target_column='Outcome')\n",
    "\n",
    "# Store original feature names before preprocessing transforms them\n",
    "original_feature_names = X_train.columns.tolist()\n",
    "\n",
    "# Create and apply preprocessing pipeline\n",
    "preprocessor_pipeline = preprocessor.create_preprocessing_pipeline(X_train)\n",
    "X_train_processed, X_val_processed, X_test_processed = preprocessor.preprocess(X_train, X_val, X_test)\n",
    "\n",
    "print(f\"Processed X_train shape: {X_train_processed.shape}\")\n",
    "print(f\"Processed X_val shape: {X_val_processed.shape}\")\n",
    "print(f\"Processed X_test shape: {X_test_processed.shape}\")\n",
    "\n",
    "# Example: Correlation analysis on original data (before scaling, for interpretability)\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(X_train.corr(), annot=False, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Matrix of Features (Training Data)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8c89e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 3. Model Training ---\")\n",
    "trainer = ModelTrainer()\n",
    "\n",
    "lr_model = trainer.train_logistic_regression(X_train_processed, y_train)\n",
    "dt_model = trainer.train_decision_tree(X_train_processed, y_train)\n",
    "\n",
    "print(\"\\nAll trained models:\", trainer.get_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a968a075",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 4. Model Evaluation and Interpretability ---\")\n",
    "\n",
    "# Combine X_test and y_test for easier SHAP access to original values for context\n",
    "original_X_test_with_target = X_test.copy()\n",
    "original_X_test_with_target['target'] = y_test # Add target back for contextual display if needed by SHAP\n",
    "\n",
    "evaluator = ModelEvaluator(feature_names=original_feature_names)\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "evaluator.evaluate_model(lr_model, X_test_processed, y_test, model_name=\"Logistic Regression\")\n",
    "evaluator.plot_feature_importance(lr_model, model_name=\"Logistic Regression\")\n",
    "# Note: SHAP for models *after* ColumnTransformer can be tricky. You might need to adjust or explain carefully.\n",
    "# For simplicity, we'll demonstrate SHAP with Decision Tree as it's more direct post-processing.\n",
    "# evaluator.plot_shap_values(lr_model, X_test_processed, original_X_test_with_target, model_name=\"Logistic Regression\")\n",
    "\n",
    "\n",
    "# Evaluate Decision Tree\n",
    "evaluator.evaluate_model(dt_model, X_test_processed, y_test, model_name=\"Decision Tree\")\n",
    "evaluator.plot_feature_importance(dt_model, model_name=\"Decision Tree\")\n",
    "evaluator.plot_shap_values(dt_model, X_test_processed, original_X_test_with_target, model_name=\"Decision Tree\") # SHAP works well here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e41f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 5. Critical Discussion and Conclusion ---\")\n",
    "\n",
    "# Discuss model performance, comparing LR and DT.\n",
    "# Which metric is most important and why? (e.g., Recall for medical diagnosis)\n",
    "# How robust are the models? What are their limitations?\n",
    "# Can these models be used in practice? How?\n",
    "# Reiterate that AI is a supporting tool, not a replacement for human experts.\n",
    "\n",
    "print(\"\"\"\n",
    "The Logistic Regression model provided a good baseline performance, demonstrating its effectiveness for this binary classification task.\n",
    "The Decision Tree Classifier, while potentially overfitting without proper tuning, offered clear feature importance, which can be valuable\n",
    "for medical professionals to understand the factors driving a diagnosis.\n",
    "\n",
    "For a medical diagnosis task like diabetes detection, 'Recall' is a paramount metric. Minimizing False Negatives (missing a diabetes case)\n",
    "is crucial, even if it comes at the cost of a slightly higher False Positive rate (healthy patients misclassified as diabetic), as the latter can\n",
    "be followed up with further diagnostic tests, while a missed case could have severe consequences.\n",
    "\n",
    "Our models show promising results, but their deployment in a real-world clinical setting would require rigorous validation,\n",
    "external testing, and integration with existing medical workflows. Furthermore, the model should only serve as an assistive tool,\n",
    "providing a preliminary assessment or highlighting areas of concern, with the final diagnostic decision always resting with a qualified medical professional.\n",
    "The interpretability provided by feature importance and SHAP values is critical for building trust and understanding among clinicians.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
